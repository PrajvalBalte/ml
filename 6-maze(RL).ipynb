{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5_-CYkH-olm"
   },
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1VyizV7a-W8u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-PbYB4F-w1y"
   },
   "source": [
    "Maze Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ac6pAl1b-0j6"
   },
   "outputs": [],
   "source": [
    "maze = [\n",
    "    [0, -1, 0, 0, 1],\n",
    "    [0, -1, 0, -1, -1],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [-1, -1, 0, -1, 0],\n",
    "    [0, 0, 0, -1, 0]\n",
    "]\n",
    "\n",
    "start = (0, 0)  # Starting point\n",
    "goal = (0, 4)   # Goal point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40kIfEg7-2dk"
   },
   "source": [
    "Q-Learning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9tMiTJTm-401"
   },
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']\n",
    "q_table = np.zeros((len(maze), len(maze[0]), len(actions)))\n",
    "alpha = 0.1     # Learning rate\n",
    "gamma = 0.9     # Discount factor\n",
    "epsilon = 0.1   # Exploration rate\n",
    "episodes = 1000 # Number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8EiVPTx-6fa"
   },
   "source": [
    "Action Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TsogS_9n-9UY"
   },
   "outputs": [],
   "source": [
    "action_dict = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGF7KOWT_A-s"
   },
   "source": [
    "Function to check if a position is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qwCKviFp_EVF"
   },
   "outputs": [],
   "source": [
    "def is_valid_position(position):\n",
    "    row, col = position\n",
    "    return 0<=row<len(maze) and 0<=col<len(maze[0]) and maze[row][col] != -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k7QjKyb_N1L"
   },
   "source": [
    "Function to choose an action (Îµ-greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "v6EbwvXQ_Qrr"
   },
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # Exploration\n",
    "    else:\n",
    "        row, col = state\n",
    "        return actions[np.argmax(q_table[row, col])]  # Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyanqEtt_Tnb"
   },
   "source": [
    "Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P76mja5m_S8P"
   },
   "outputs": [],
   "source": [
    "for episode in range(episodes):\n",
    "    state = start\n",
    "    while state != goal:\n",
    "        row, col = state\n",
    "        action = choose_action(state)\n",
    "        move = action_dict[action]\n",
    "        next_state = (row + move[0], col + move[1])\n",
    "\n",
    "        if not is_valid_position(next_state):\n",
    "            reward = -1  # Penalty for hitting a wall\n",
    "            next_state = state  # Stay in the same position\n",
    "        elif next_state == goal:\n",
    "            reward = 1  # Reward for reaching the goal\n",
    "        else:\n",
    "            reward = -0.1  # Small penalty for each move\n",
    "\n",
    "        # Update Q-Value\n",
    "        next_row, next_col = next_state\n",
    "        best_next_action = np.max(q_table[next_row, next_col])\n",
    "        q_table[row, col, actions.index(action)] += alpha * (reward + gamma * best_next_action - q_table[row, col, actions.index(action)])\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "    # Decrease exploration rate over time\n",
    "    epsilon = max(0.01, epsilon * 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY2CZOKF_Z1P"
   },
   "source": [
    "Display the learned Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "2aMRrvep_bCj",
    "outputId": "badf0360-f32e-43f1-d084-a5592f0b122a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-Table:\n",
      "[[[-0.66834614 -0.0434062  -0.68690074 -0.65862256]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.28217252  0.01544064 -0.22633692  0.8       ]\n",
      "  [-0.15592538 -0.10904868  0.22995088  1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.25253409  0.062882   -0.60845351 -0.51489876]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.62        0.02233587 -0.27946679 -0.17050279]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.24594887 -0.37915319 -0.56801193  0.18098   ]\n",
      "  [-0.25619442 -0.46858785 -0.16200752  0.3122    ]\n",
      "  [ 0.458      -0.14526511 -0.01069267 -0.14891004]\n",
      "  [-0.199      -0.199      -0.04194172 -0.13867746]\n",
      "  [-0.199      -0.13896771 -0.14110972 -0.28843942]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.03810066 -0.12992155 -0.199      -0.199     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.14056829 -0.13974572 -0.199      -0.28436127]]\n",
      "\n",
      " [[-0.199      -0.199      -0.28266994 -0.1248566 ]\n",
      "  [-0.199      -0.199      -0.12261554 -0.13126355]\n",
      "  [-0.13124879 -0.27792311 -0.12877831 -0.199     ]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [-0.14044039 -0.1909     -0.29064487 -0.199     ]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained Q-Table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxGa5gmI_bou"
   },
   "source": [
    "Testing the agent's learned policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5lJn9P7O_ewt",
    "outputId": "a96eb927-37ab-424b-9383-89a34af3333a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path taken by the agent: [(0, 0), (1, 0), (2, 0), (2, 1), (2, 2), (1, 2), (0, 2), (0, 3), (0, 4)]\n"
     ]
    }
   ],
   "source": [
    "state = start\n",
    "path = [state]\n",
    "while state != goal:\n",
    "    row, col = state\n",
    "    action = actions[np.argmax(q_table[row, col])]\n",
    "    move = action_dict[action]\n",
    "    next_state = (row + move[0], col + move[1])\n",
    "    if not is_valid_position(next_state):\n",
    "        break\n",
    "    state = next_state\n",
    "    path.append(state)\n",
    "\n",
    "print(\"Path taken by the agent:\", path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
